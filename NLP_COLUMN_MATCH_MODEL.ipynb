{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "vta2gnzowqs4okd66mff",
   "authorId": "5574880875019",
   "authorName": "TBOON",
   "authorEmail": "terry.boon@snowflake.com",
   "sessionId": "a529c126-530f-4454-983c-d3be3ddd82eb",
   "lastEditTime": 1764761822841
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869e256c-3e46-47f8-9b6e-21108ae9a2f9",
   "metadata": {
    "name": "Doc1",
    "collapsed": false
   },
   "source": "# Healthcare Field Mappings Setup\n\nThis section initializes the `mappings_list` table with standard healthcare and insurance field names. The table serves as a reference for field mapping and data standardization.\n\n## Table Structure\n- **src**: Source field name\n- **target**: Target field name (standardized)\n\n## Field Categories Included:\n\n### Policy & Member Information\n- Policy Effective Date\n- Group Name\n- Insured First Name, Last Name, DOB\n- Claimant First Name, Last Name, DOB\n\n### Service & Claims Data\n- Beginning/Ending Service Date\n- Processed Date\n- Claim Number/Claim Control Number\n- Service Line/Claim Type\n\n### Medical Coding\n- Primary ICD, Secondary ICD\n- CPT Code, HCPCS Code\n- Revenue Code, Modifier Code\n\n### Prescription Information\n- Rx Name, Quantity, Days Supply\n- Rx Date Filled\n\n### Financial Amounts\n- Billed Amount, Allowed Amount, Paid Amount\n- Copay, Deductible, Coinsurance Amount\n- Net, Ineligible, COB, Other Reduced, Denied Amount\n\n### Provider Information\n- NPI (National Provider Identifier)\n- Payee Name, Address, TIN\n\n## Usage\nThis mapping table will be used by the FieldMatcher model to automatically match incoming field names to standardized target field names, enabling consistent data processing across different healthcare data sources."
  },
  {
   "cell_type": "code",
   "id": "04964cda-e3f6-4d99-bb75-cdcb14828b9b",
   "metadata": {
    "language": "sql",
    "name": "DataSetup"
   },
   "outputs": [],
   "source": "--DROP TABLE mappings_list; --If you want to start from scratch\nCREATE TABLE IF NOT EXISTS mappings_list (\n    src VARCHAR,\n    target VARCHAR\n);\n\n-- Use MERGE to avoid duplicates\nMERGE INTO mappings_list AS target\nUSING (\n    VALUES\n    ('Policy Effective Date', 'Policy Effective Date'),\n    ('Group Name', 'Group Name'),\n    ('Insured First Name', 'Insured First Name'),\n    ('Insured Last Name', 'Insured Last Name'),\n    ('Insured DOB', 'Insured DOB'),\n    ('Claimant First Name', 'Claimant First Name'),\n    ('Claimant Last Name', 'Claimant Last Name'),\n    ('Claimant DOB', 'Claimant DOB'),\n    ('Beginning Service Date', 'Beginning Service Date'),\n    ('Claim Number/Claim Control Number', 'Claim Number/Claim Control Number'),\n    ('Ending Service Date', 'Ending Service Date'),\n    ('Processed Date', 'Processed Date'),\n    ('Primary ICD', 'Primary ICD'),\n    ('Secondary ICD', 'Secondary ICD'),\n    ('CPT Code', 'CPT Code'),\n    ('HCPCS Code', 'HCPCS Code'),\n    ('Revenue Code', 'Revenue Code'),\n    ('Modifier Code', 'Modifier Code'),\n    ('Product Type', 'Product Type'),\n    ('NPI?', 'NPI?'),\n    ('Rx Name', 'Rx Name'),\n    ('Rx Quantity', 'Rx Quantity'),\n    ('Rx Days Supply', 'Rx Days Supply'),\n    ('Rx Date Filled', 'Rx Date Filled'),\n    ('Billed Amount', 'Billed Amount'),\n    ('Copay Amount', 'Copay Amount'),\n    ('Deductible Amount', 'Deductible Amount'),\n    ('Coinsurance Amount', 'Coinsurance Amount'),\n    ('Allowed Amount', 'Allowed Amount'),\n    ('Net Amount', 'Net Amount'),\n    ('Ineligible Amount', 'Ineligible Amount'),\n    ('COB Amount', 'COB Amount'),\n    ('Other Reduced Amount', 'Other Reduced Amount'),\n    ('Denied Amount', 'Denied Amount'),\n    ('Paid Amount', 'Paid Amount'),\n    ('Service Line/Claim Type', 'Service Line/Claim Type'),\n    ('Payee Name', 'Payee Name'),\n    ('Payee Address', 'Payee Address'),\n    ('Payee TIN', 'Payee TIN')\n) AS source (src, target)\nON target.src = source.src AND target.target = source.target\nWHEN NOT MATCHED THEN\n    INSERT (src, target) VALUES (source.src, source.target);\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35274bdd-1d84-4874-985e-84f9fb59353b",
   "metadata": {
    "language": "python",
    "name": "MappingModel",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Healthcare Field Matching System - CORRECTED VERSION\n# This module provides intelligent field name matching using multiple similarity algorithms\n\nfrom snowflake.snowpark.context import get_active_session\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nfrom difflib import SequenceMatcher\nimport re\n\n# Initialize Snowpark session for database connectivity\nsession = get_active_session()\n\n# Load the mappings data from Snowflake table\nmappings_df = session.sql(\"SELECT * FROM mappings_list\").to_pandas()\n\nclass FieldMatcher:\n    \"\"\"\n    Healthcare Field Matching System\n    \n    This class provides intelligent field name matching using multiple similarity algorithms\n    to map incoming field names to standardized healthcare field names.\n    \n    Features:\n    - Exact matching for identical field names\n    - Substring matching for partial matches\n    - Sequence similarity using difflib for character-level similarity\n    - Word overlap using Jaccard similarity coefficient\n    - TF-IDF vectorization with cosine similarity for semantic matching\n    \n    Attributes:\n        mappings_df (DataFrame): Pandas DataFrame containing source and target field mappings\n        src_fields (list): List of source field names to match against\n        target_fields (list): List of standardized target field names\n        src_to_target (dict): Dictionary mapping source fields to target fields\n        tfidf (TfidfVectorizer): Scikit-learn TF-IDF vectorizer for semantic analysis\n        tfidf_matrix: Fitted TF-IDF matrix of source field names\n    \"\"\"\n    \n    def __init__(self, mappings_df):\n        \"\"\"\n        Initialize the FieldMatcher with mapping data\n        \n        Args:\n            mappings_df (DataFrame): DataFrame with 'SRC' and 'TARGET' columns\n        \"\"\"\n        self.mappings_df = mappings_df\n        # Match against source fields, not target fields\n        self.src_fields = mappings_df['SRC'].tolist()\n        self.target_fields = mappings_df['TARGET'].tolist()\n        \n        # Create mapping dictionary for quick lookup\n        self.src_to_target = dict(zip(mappings_df['SRC'], mappings_df['TARGET']))\n        \n        # Initialize TF-IDF vectorizer for semantic similarity\n        # Uses 1-3 word n-grams to capture multi-word field names\n        self.tfidf = TfidfVectorizer(\n            lowercase=True,           # Convert all text to lowercase\n            ngram_range=(1, 3),      # Use 1-3 word combinations for better matching\n            max_features=1000        # Limit vocabulary size for performance\n        )\n        \n        # Pre-fit vectorizer on source fields (not target fields)\n        self.tfidf_matrix = self.tfidf.fit_transform(self.src_fields)\n    \n    def preprocess_text(self, text):\n        \"\"\"\n        Clean and normalize text for consistent matching\n        \n        This function standardizes input text by:\n        - Converting to lowercase\n        - Removing special characters and punctuation\n        - Normalizing whitespace\n        \n        Args:\n            text (str): Input text to preprocess\n            \n        Returns:\n            str: Cleaned and normalized text\n        \"\"\"\n        if not text:\n            return \"\"\n        # Convert to lowercase and remove special characters (keep only alphanumeric and spaces)\n        text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n        # Remove extra whitespace and normalize spacing\n        text = ' '.join(text.split())\n        return text\n    \n    def calculate_similarity_scores(self, input_text):\n        \"\"\"\n        Calculate multiple similarity scores between input text and all source fields\n        \n        This method computes four different similarity metrics:\n        1. Exact Match: Binary score for identical strings after preprocessing\n        2. Substring Match: Binary score if one string contains the other\n        3. Sequence Similarity: Character-level similarity using difflib\n        4. Word Overlap: Jaccard similarity coefficient of word sets\n        \n        Args:\n            input_text (str): Field name to match against source fields\n            \n        Returns:\n            list: List of dictionaries containing scores for each source field\n        \"\"\"\n        input_text = self.preprocess_text(input_text)\n        scores = []\n        \n        # Compare against source fields, not target fields\n        for src_field in self.src_fields:\n            src_clean = self.preprocess_text(src_field)\n            \n            # 1. Exact match score - highest confidence when strings are identical\n            exact_score = 1.0 if input_text == src_clean else 0.0\n            \n            # 2. Substring match score - high confidence for partial matches\n            substring_score = 1.0 if input_text in src_clean or src_clean in input_text else 0.0\n            \n            # 3. Sequence similarity using difflib - handles typos and variations\n            sequence_score = SequenceMatcher(None, input_text, src_clean).ratio()\n            \n            # 4. Word overlap score using Jaccard similarity\n            # Measures overlap between sets of words in each field name\n            input_words = set(input_text.split())\n            src_words = set(src_clean.split())\n            if len(input_words.union(src_words)) > 0:\n                word_overlap = len(input_words.intersection(src_words)) / len(input_words.union(src_words))\n            else:\n                word_overlap = 0.0\n            \n            # Combine scores with weighted average\n            # Weights: Exact (40%), Substring (20%), Sequence (20%), Word Overlap (20%)\n            combined_score = (\n                exact_score * 0.4 +\n                substring_score * 0.2 +\n                sequence_score * 0.2 +\n                word_overlap * 0.2\n            )\n            \n            # Store all scores for analysis and debugging\n            # Return the corresponding target field for the matched source field\n            scores.append({\n                'src_field': src_field,\n                'target_field': self.src_to_target[src_field],\n                'combined_score': combined_score,\n                'exact_score': exact_score,\n                'substring_score': substring_score,\n                'sequence_score': sequence_score,\n                'word_overlap': word_overlap\n            })\n        \n        return scores\n    \n    def predict_match(self, input_text, top_n=3, min_threshold=0.1):\n        \"\"\"\n        Predict the best matching target field(s) for input text\n        Matches against SRC fields but returns TARGET field names\n        \n        This method combines multiple similarity algorithms with TF-IDF semantic matching\n        to provide ranked predictions with confidence scores.\n        \n        Args:\n            input_text (str): Field name to match\n            top_n (int): Maximum number of predictions to return (default: 3)\n            min_threshold (float): Minimum score threshold for predictions (default: 0.1)\n            \n        Returns:\n            list: Sorted list of prediction dictionaries with scores and field names\n        \"\"\"\n        # Get basic similarity scores (comparing against SRC fields)\n        scores = self.calculate_similarity_scores(input_text)\n        \n        # Add TF-IDF cosine similarity for semantic matching (against SRC fields)\n        # This helps match fields with similar meaning but different wording\n        input_vector = self.tfidf.transform([self.preprocess_text(input_text)])\n        cosine_scores = cosine_similarity(input_vector, self.tfidf_matrix)[0]\n        \n        # Enhance combined scores with TF-IDF semantic similarity\n        for i, score_dict in enumerate(scores):\n            score_dict['tfidf_score'] = cosine_scores[i]\n            # Update combined score: 70% basic similarity + 30% semantic similarity\n            score_dict['combined_score'] = (\n                score_dict['combined_score'] * 0.7 + \n                cosine_scores[i] * 0.3\n            )\n        \n        # Sort by combined score (highest confidence first)\n        scores = sorted(scores, key=lambda x: x['combined_score'], reverse=True)\n        \n        # Filter by minimum threshold and return top N predictions\n        filtered_scores = [s for s in scores if s['combined_score'] >= min_threshold]\n        \n        return filtered_scores[:top_n]\n    \n    def batch_predict(self, input_list, top_n=1):\n        \"\"\"\n        Predict matches for multiple input values efficiently\n        \n        This method processes a list of field names and returns the best match\n        for each input along with confidence scores.\n        \n        Args:\n            input_list (list): List of field names to match\n            top_n (int): Number of predictions per input (default: 1 for best match only)\n            \n        Returns:\n            list: List of prediction results with input, predicted target, and confidence\n        \"\"\"\n        results = []\n        for input_text in input_list:\n            predictions = self.predict_match(input_text, top_n=top_n)\n            # Get best match or indicate no match found\n            best_match = predictions[0] if predictions else {'target_field': 'NO_MATCH', 'combined_score': 0.0}\n            results.append({\n                'input': input_text,\n                'predicted_target': best_match['target_field'],\n                'confidence_score': best_match['combined_score']\n            })\n        return results",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8a999e2-06ff-4bcc-a321-217615e25d42",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "\n# Initialize the matcher\nmatcher = FieldMatcher(mappings_df)\n\n# Test the model with some examples\ntest_inputs = [\n    \"amt\",\n    \"Policy Effective Date\",\n    \"Policy Start Date\",\n    \"Member First Name\", \n    \"Patient DOB\",\n    \"Claim ID\",\n    \"Service Begin Date\",\n    \"Diagnosis Code\",\n    \"Payment Amount\",\n    \"Provider NPI\"\n]\n\nprint(\"Testing the Field Matching Model:\")\nprint(\"=\" * 50)\n\nfor test_input in test_inputs:\n    predictions = matcher.predict_match(test_input, top_n=3)\n    print(f\"\\nInput: '{test_input}'\")\n    print(\"Top 3 Predictions:\")\n    \n    if predictions:\n        for i, pred in enumerate(predictions, 1):\n            print(f\"  {i}. {pred['target_field']} (Score: {pred['combined_score']:.3f})\")\n    else:\n        print(\"  No good matches found\")\n\n# Create a function for easy prediction\ndef predict_field_mapping(input_text):\n    \"\"\"Simple function to get the best field mapping prediction\"\"\"\n    predictions = matcher.predict_match(input_text, top_n=1)\n    if predictions and predictions[0]['combined_score'] > 0.3:  # Confidence threshold\n        return predictions[0]['target_field']\n    else:\n        return \"NO_CONFIDENT_MATCH\"\n\nprint(f\"\\n\\nQuick prediction examples:\")\nprint(f\"'Patient Name' -> {predict_field_mapping('Patient Name')}\")\nprint(f\"'Birth Date' -> {predict_field_mapping('Birth Date')}\")\nprint(f\"'Claim Control Number' -> {predict_field_mapping('Claim Control Number')}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd3b216e-6fea-4d68-8afe-5a3d0959e4d5",
   "metadata": {
    "language": "sql",
    "name": "pythonSPs"
   },
   "outputs": [],
   "source": "-- Corrected Field Matcher Procedure - Matches SRC fields, Returns TARGET fields\nCREATE OR REPLACE PROCEDURE field_matcher_advanced(\n    input_fields ARRAY,\n    top_n INTEGER DEFAULT 3,\n    min_threshold FLOAT DEFAULT 0.1\n)\nRETURNS TABLE (\n    input_field STRING,\n    src_field STRING,\n    target_field STRING,\n    combined_score FLOAT,\n    exact_score FLOAT,\n    substring_score FLOAT,\n    sequence_score FLOAT,\n    word_overlap FLOAT,\n    tfidf_score FLOAT,\n    match_rank INTEGER\n)\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.11'\nPACKAGES = ('numpy', 'scikit-learn', 'snowflake-snowpark-python')\nHANDLER = 'run'\nAS\n$$\nimport re\nimport numpy as np\nfrom difflib import SequenceMatcher\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.types import StructType, StructField, StringType, FloatType, IntegerType\n\ndef preprocess_text(text):\n    \"\"\"Clean and normalize text for better matching\"\"\"\n    if not text:\n        return \"\"\n    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n    return ' '.join(text.split())\n\ndef calculate_similarity_scores(input_text, src_fields, src_to_target_map):\n    \"\"\"Calculate various similarity scores for the input text against all source fields\"\"\"\n    input_clean = preprocess_text(input_text)\n    scores = []\n    \n    for src_field in src_fields:\n        src_clean = preprocess_text(src_field)\n        \n        # 1. Exact match score\n        exact_score = 1.0 if input_clean == src_clean else 0.0\n        \n        # 2. Substring match score\n        substring_score = 1.0 if (input_clean in src_clean or src_clean in input_clean) else 0.0\n        \n        # 3. Sequence similarity (difflib)\n        sequence_score = SequenceMatcher(None, input_clean, src_clean).ratio()\n        \n        # 4. Word overlap score\n        input_words = set(input_clean.split()) if input_clean else set()\n        src_words = set(src_clean.split()) if src_clean else set()\n        \n        if len(input_words.union(src_words)) > 0:\n            word_overlap = len(input_words.intersection(src_words)) / len(input_words.union(src_words))\n        else:\n            word_overlap = 0.0\n        \n        # Initial combined score (before TF-IDF)\n        combined_score = (\n            exact_score * 0.4 +\n            substring_score * 0.2 +\n            sequence_score * 0.2 +\n            word_overlap * 0.2\n        )\n        \n        scores.append({\n            'src_field': src_field,\n            'target_field': src_to_target_map.get(src_field, 'UNMAPPED'),\n            'combined_score': combined_score,\n            'exact_score': exact_score,\n            'substring_score': substring_score,\n            'sequence_score': sequence_score,\n            'word_overlap': word_overlap,\n            'tfidf_score': 0.0  # Will be updated later\n        })\n    \n    return scores\n\ndef add_tfidf_scores(input_text, scores, src_fields):\n    \"\"\"Add TF-IDF cosine similarity to the scores\"\"\"\n    try:\n        # Initialize TF-IDF vectorizer\n        tfidf = TfidfVectorizer(\n            lowercase=True,\n            ngram_range=(1, 3),\n            max_features=1000\n        )\n        \n        # Prepare text data - preprocess source fields\n        preprocessed_src_fields = [preprocess_text(field) for field in src_fields]\n        preprocessed_input = preprocess_text(input_text)\n        \n        # Fit vectorizer on source fields (not target fields)\n        tfidf_matrix = tfidf.fit_transform(preprocessed_src_fields)\n        \n        # Transform input text\n        input_vector = tfidf.transform([preprocessed_input])\n        \n        # Calculate cosine similarity\n        cosine_scores = cosine_similarity(input_vector, tfidf_matrix)[0]\n        \n        # Update scores with TF-IDF\n        for i, score_dict in enumerate(scores):\n            score_dict['tfidf_score'] = float(cosine_scores[i])\n            # Update combined score: 70% basic similarity + 30% semantic similarity\n            score_dict['combined_score'] = (\n                score_dict['combined_score'] * 0.7 + \n                cosine_scores[i] * 0.3\n            )\n            \n    except Exception as e:\n        # If TF-IDF fails, use original scores\n        for score_dict in scores:\n            score_dict['tfidf_score'] = 0.0\n    \n    return scores\n\ndef run(session, input_fields, top_n, min_threshold):\n    # Get both SRC and TARGET fields from mappings table\n    mappings_query = \"\"\"\n    SELECT SRC, TARGET FROM mappings_list ORDER BY SRC\n    \"\"\"\n    \n    # Execute the query and get results\n    mappings_result = session.sql(mappings_query).collect()\n    \n    # Create lists and mapping dictionary\n    src_fields = [row[0] for row in mappings_result]\n    src_to_target_map = {row[0]: row[1] for row in mappings_result}\n    \n    results = []\n    \n    # Process each input field\n    for input_field in input_fields:\n        # Calculate similarity scores against SOURCE fields\n        field_scores = calculate_similarity_scores(input_field, src_fields, src_to_target_map)\n        \n        # Add TF-IDF scores (calculated against SOURCE fields)\n        field_scores = add_tfidf_scores(input_field, field_scores, src_fields)\n        \n        # Sort by combined score\n        field_scores = sorted(field_scores, key=lambda x: x['combined_score'], reverse=True)\n        \n        # Filter by minimum threshold and get top N\n        filtered_scores = [s for s in field_scores if s['combined_score'] >= min_threshold][:top_n]\n        \n        # Add to results with ranking\n        for rank, score_dict in enumerate(filtered_scores, 1):\n            results.append([\n                input_field,                                    # input_field\n                score_dict['src_field'],                       # src_field (what was matched)\n                score_dict['target_field'],                    # target_field (what gets returned)\n                round(score_dict['combined_score'], 4),        # combined_score\n                round(score_dict['exact_score'], 4),           # exact_score\n                round(score_dict['substring_score'], 4),       # substring_score\n                round(score_dict['sequence_score'], 4),        # sequence_score\n                round(score_dict['word_overlap'], 4),          # word_overlap\n                round(score_dict['tfidf_score'], 4),           # tfidf_score\n                rank                                           # match_rank\n            ])\n    \n    # Define schema for the DataFrame\n    schema = StructType([\n        StructField(\"input_field\", StringType()),\n        StructField(\"src_field\", StringType()),\n        StructField(\"target_field\", StringType()),\n        StructField(\"combined_score\", FloatType()),\n        StructField(\"exact_score\", FloatType()),\n        StructField(\"substring_score\", FloatType()),\n        StructField(\"sequence_score\", FloatType()),\n        StructField(\"word_overlap\", FloatType()),\n        StructField(\"tfidf_score\", FloatType()),\n        StructField(\"match_rank\", IntegerType())\n    ])\n    \n    # Create and return Snowpark DataFrame\n    return session.create_dataframe(results, schema)\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00bff763-9280-41c0-b11f-76b52f848693",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "-- Test the advanced procedure with multiple fields\nCALL field_matcher_advanced(\n    ['amt_allowed', 'Patient Name', 'Birth Date', 'Policy Start Date', 'total_amt', 'Diagnosis Code'], \n    3, \n    0.1\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "860993b3-ccd9-4e4d-bb60-3b3b601f83c6",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "-- Simplified version that takes target fields as parameter\nCREATE OR REPLACE FUNCTION field_matcher_predict_simple(\n    input_field_name STRING,\n    target_fields_array ARRAY,\n    min_threshold FLOAT DEFAULT 0.3\n)\nRETURNS STRING\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.11'\nHANDLER = 'run'\nAS\n$$\nimport re\nfrom difflib import SequenceMatcher\n\ndef preprocess_text(text):\n    if not text:\n        return \"\"\n    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n    return ' '.join(text.split())\n\ndef calculate_combined_score(input_text, target_text):\n    input_clean = preprocess_text(input_text)\n    target_clean = preprocess_text(target_text)\n    \n    # Basic similarity scores\n    exact_score = 1.0 if input_clean == target_clean else 0.0\n    substring_score = 1.0 if (input_clean in target_clean or target_clean in input_clean) else 0.0\n    sequence_score = SequenceMatcher(None, input_clean, target_clean).ratio()\n    \n    input_words = set(input_clean.split()) if input_clean else set()\n    target_words = set(target_clean.split()) if target_clean else set()\n    \n    if len(input_words.union(target_words)) > 0:\n        word_overlap = len(input_words.intersection(target_words)) / len(input_words.union(target_words))\n    else:\n        word_overlap = 0.0\n    \n    return (exact_score * 0.4 + substring_score * 0.2 + \n            sequence_score * 0.2 + word_overlap * 0.2)\n\ndef run(input_field_name, target_fields_array, min_threshold):\n    best_score = 0.0\n    best_match = \"NO_MATCH\"\n    \n    # Find best match using basic similarity\n    for target_field in target_fields_array:\n        score = calculate_combined_score(input_field_name, target_field)\n        if score > best_score and score >= min_threshold:\n            best_score = score\n            best_match = target_field\n    \n    return best_match\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57320388-0db8-47e3-8075-0ca06728f064",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- Test simple function with target fields array\nWITH target_list AS (\n    SELECT ARRAY_AGG(DISTINCT TARGET) as targets \n    FROM mappings_list\n)\nSELECT \n    field_matcher_predict_simple('Patient First Name', targets, 0.2) as match1,\n    field_matcher_predict_simple('DOB', targets, 0.2) as match2,\n    field_matcher_predict_simple('Claim ID', targets, 0.2) as match3\nFROM target_list;",
   "execution_count": null
  }
 ]
}